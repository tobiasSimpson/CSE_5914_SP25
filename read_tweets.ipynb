{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets from sanders dataset\n",
    "def load_sanders_tweets(filename: str = 'sanders_corpus.csv') -> pd.DataFrame:\n",
    "    filepath = f\"{DATA_FOLDER}/{filename}\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop(columns=['TweetId', 'Sentiment'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df['TweetDate'] = pd.to_datetime(df['TweetDate'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets from celebrities\n",
    "def load_celebrity_tweets(filename: str = 'celebrity_tweets.csv') -> pd.DataFrame:\n",
    "    column_names = ['user', 'tweet', 'sentiment_label']\n",
    "    df = pd.read_csv(f'{DATA_FOLDER}/{filename}', header=None, names=column_names)\n",
    "    df.drop(columns=['sentiment_label'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cikm_tweets(filename: str = 'cikm_2010_tweets.txt') -> pd.DataFrame:\n",
    "    # helper to process a single file line\n",
    "    def process_line(line):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) != 4:\n",
    "            return None\n",
    "        user_id, tweet_id, tweet, created_at = parts\n",
    "        # return {'Tweet': tweet, 'CreatedAt': created_at}\n",
    "        return (tweet, created_at)\n",
    "    \n",
    "    # read file and process each line:\n",
    "    file_lines = open(f'{DATA_FOLDER}/{filename}').readlines()\n",
    "    results = []\n",
    "    for line in tqdm(file_lines):\n",
    "        result = process_line(line)\n",
    "        if result:results.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=['Tweet', 'CreatedAt'])\n",
    "    # read created at column as datetime, and drop any row with invalid values:\n",
    "    df['CreatedAt'] = pd.to_datetime(df['CreatedAt'], errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kaggle_tweets(filename: str = 'kaggle_tweets.csv') -> pd.DataFrame:\n",
    "    column_names = ['sentiment', 'tweet_id', 'date', 'query_status', 'username', 'text']\n",
    "    # helper to process a single file line\n",
    "    def process_line(line):\n",
    "        parts = line.strip().split('\",\"')\n",
    "        parts = list(map(lambda x: x.replace('\"', ''), parts))\n",
    "        if len(parts) != len(column_names):\n",
    "            return None\n",
    "        return parts\n",
    "    \n",
    "    # read file and process each line:\n",
    "    file_lines = open(f'{DATA_FOLDER}/{filename}', encoding='latin-1').readlines()\n",
    "    results = []\n",
    "    for line in tqdm(file_lines):\n",
    "        result = process_line(line)\n",
    "        if result:results.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "    # read created at column as datetime, and drop any row with invalid values:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str) -> pd.DataFrame:\n",
    "    func_mapping = {\n",
    "        'sanders': load_sanders_tweets,\n",
    "        'celebrity': load_celebrity_tweets,\n",
    "        'cikm': load_cikm_tweets,\n",
    "        'kaggle': load_kaggle_tweets\n",
    "    }\n",
    "    assert name in func_mapping, f\"Dataset {name} not found.\"\n",
    "    return func_mapping[name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65626/1974470176.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['TweetDate'] = pd.to_datetime(df['TweetDate'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5113 tweets from sanders dataset.\n",
      "   Topic                 TweetDate  \\\n",
      "0  apple 2011-10-18 21:53:25+00:00   \n",
      "1  apple 2011-10-18 21:09:33+00:00   \n",
      "2  apple 2011-10-18 21:02:20+00:00   \n",
      "3  apple 2011-10-18 20:40:10+00:00   \n",
      "4  apple 2011-10-18 20:34:00+00:00   \n",
      "\n",
      "                                           TweetText  \n",
      "0  Now all @Apple has to do is get swype on the i...  \n",
      "1  @Apple will be adding more carrier support to ...  \n",
      "2  Hilarious @youtube video - guy does a duet wit...  \n",
      "3  @RIM you made it too easy for me to switch to ...  \n",
      "4  I just realized that the reason I got into twi...  \n",
      "Loaded 3215 tweets from celebrity dataset.\n",
      "          user                                              tweet\n",
      "0  BarackObama  Aretha helped define the American experience. ...\n",
      "1  BarackObama  Bobby Kennedy was one of my heroes. He was som...\n",
      "2  BarackObama  I’m confident that, together, they’ll strength...\n",
      "3  BarackObama  Today I’m proud to endorse such a wide and imp...\n",
      "4  BarackObama  Mandela Day is about taking action to change t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9001672/9001672 [00:09<00:00, 984628.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8783191 tweets from cikm dataset.\n",
      "                                               Tweet           CreatedAt\n",
      "0  Ok today I have to find something to wear for ... 2010-03-15 17:35:58\n",
      "1  I am glad I'm having this show but I can't wai... 2010-03-15 16:53:44\n",
      "2  Honestly I don't even know what's going on any... 2010-03-15 16:52:59\n",
      "3  @LovelyJ_Janelle hey sorry I'm sitting infront... 2010-03-15 15:42:07\n",
      "4  Sitting infront of this sewing machine ... I d... 2010-03-15 13:55:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [00:07<00:00, 217314.69it/s]\n",
      "/tmp/ipykernel_65626/756693504.py:20: FutureWarning: Parsed string \"Mon Apr 06 22:19:45 PDT 2009\" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1600000 tweets from kaggle dataset.\n",
      "  sentiment    tweet_id                date query_status         username  \\\n",
      "0         0  1467810369 2009-04-06 22:19:45     NO_QUERY  _TheSpecialOne_   \n",
      "1         0  1467810672 2009-04-06 22:19:49     NO_QUERY    scotthamilton   \n",
      "2         0  1467810917 2009-04-06 22:19:53     NO_QUERY         mattycus   \n",
      "3         0  1467811184 2009-04-06 22:19:57     NO_QUERY          ElleCTF   \n",
      "4         0  1467811193 2009-04-06 22:19:57     NO_QUERY           Karoli   \n",
      "\n",
      "                                                text  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    datasets = ['sanders', 'celebrity', 'cikm', 'kaggle']\n",
    "    for dataset in datasets:\n",
    "        # print(f\"Loading {dataset} tweets...\")\n",
    "        df = load_dataset(dataset)\n",
    "        print(f\"Loaded {len(df)} tweets from {dataset} dataset.\")\n",
    "        print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5914",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
